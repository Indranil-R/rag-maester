{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Indranil-R/rag-maester/blob/master/rag_maester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f875d2fa-8df2-4951-9bfb-87b71c8b5e1b",
      "metadata": {
        "id": "f875d2fa-8df2-4951-9bfb-87b71c8b5e1b"
      },
      "source": [
        "<!-- ![](assets/img/image.png) -->\n",
        "## RAG Maester\n",
        "**Your AI Scholar**\n",
        "\n",
        "Welcome to **RAG Maester**, an Academic AI assistant designed to support academic excellence.\n",
        "It leverages **Retrieval Augmented Generation (RAG)** to meticulously search its knowledge base and craft well-informed responses, designed to assist with university assignments and tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "SbNoeCZ-IEFR",
      "metadata": {
        "id": "SbNoeCZ-IEFR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "QbdP1TauEu-D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QbdP1TauEu-D",
        "outputId": "88265df8-2988-4778-f9e8-11a986b97ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirements.txt already exists. Downloading modules...\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Downloading the required modules\n",
        "if os.path.isfile(\"requirements.txt\"):\n",
        "  print(\"Requirements.txt already exists. Downloading modules...\")\n",
        "else:\n",
        "  print(\"Requirements.txt doesn't exist downloading from github...\")\n",
        "  url = 'https://raw.githubusercontent.com/Indranil-R/rag-maester/refs/heads/master/requirements.txt'\n",
        "  response = requests.get(url)\n",
        "\n",
        "  with open('requirements.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(response.text)\n",
        "  print(\"File downloaded successfully.\")\n",
        "\n",
        "# !pip install -q -r requirements.txt  # Enable it only if dependencies are not installed, I have installed already"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac42ccfa-7fdf-44ff-9e93-aabc88c74453",
      "metadata": {
        "id": "ac42ccfa-7fdf-44ff-9e93-aabc88c74453"
      },
      "source": [
        "## Importing all required third party libraries\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3e896eba-e5de-45a3-89e2-0de1167d6222",
      "metadata": {
        "id": "3e896eba-e5de-45a3-89e2-0de1167d6222"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    from google.colab import userdata\n",
        "else:\n",
        "    # do nothing\n",
        "    pass\n",
        "\n",
        "\n",
        "from loguru import logger\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d-QNUpRJMLqf",
      "metadata": {
        "id": "d-QNUpRJMLqf"
      },
      "outputs": [],
      "source": [
        "# Setting up Google API key\n",
        "if os.getenv('GOOGLE_API_KEY') == None:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d42d5fee-8773-47c9-a070-80aae4524516",
      "metadata": {
        "id": "d42d5fee-8773-47c9-a070-80aae4524516"
      },
      "source": [
        "## 1. Upload and Ingest Documents ðŸ“„"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42271c07-9995-4d18-a2e4-5b2abb3c7088",
      "metadata": {
        "id": "42271c07-9995-4d18-a2e4-5b2abb3c7088"
      },
      "source": [
        "### Scan the docs directory for all available documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b0c0b93-1cf3-409f-993c-10b0bf91b43f",
      "metadata": {
        "id": "0b0c0b93-1cf3-409f-993c-10b0bf91b43f",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Fetch all file paths from a directory\n",
        "\n",
        "def fetch_all_docs(docs_path: str) -> list[str]:\n",
        "    docs_list = []\n",
        "    if not os.path.isdir(docs_path):\n",
        "        print(f\"Warning: The path '{docs_path}' is not a valid directory or does not exist.\")\n",
        "        return []\n",
        "    try:\n",
        "        for item_name in os.listdir(docs_path):\n",
        "            item_full_path = os.path.join(docs_path, item_name)\n",
        "            if os.path.isfile(item_full_path):\n",
        "                docs_list.append(item_full_path)\n",
        "    except OSError as e:\n",
        "        logger.error(f\"Error accessing or reading directory '{docs_path}': {e}\")\n",
        "        return []\n",
        "    return docs_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "962f7cfa-8f7f-4dba-8a1d-36b2b30730bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "962f7cfa-8f7f-4dba-8a1d-36b2b30730bb",
        "outputId": "28996d6c-5e60-4a91-eabb-4bfa9e1b8c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-05-17 01:08:05.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mTotal number of documents found: 3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Fetching all documents from the docs directory\n",
        "documents_list = fetch_all_docs(os.getcwd() + \"/docs\")\n",
        "\n",
        "logger.info(f\"Total number of documents found: {len(documents_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fc996afb95cabb",
      "metadata": {
        "id": "73fc996afb95cabb"
      },
      "source": [
        "#### Split the documents into smaller chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c4602da5-1e12-461c-ab02-dedd3101f839",
      "metadata": {
        "id": "c4602da5-1e12-461c-ab02-dedd3101f839",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Clean text by removing predefined phrases\n",
        "\n",
        "def clean_text(text):\n",
        "    removal_phrases = [\n",
        "        \"(c) Amity University Online\",\n",
        "        \"Notes\",\n",
        "        \"Amity Directorate of Distance & Online Education\",\n",
        "        \"Introduction to E-Governance\"\n",
        "    ]\n",
        "    for phrase in removal_phrases:\n",
        "        text = text.replace(phrase, \"\")\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f00ca8da-23c6-4f4f-92a8-0fdb1a25b547",
      "metadata": {
        "id": "f00ca8da-23c6-4f4f-92a8-0fdb1a25b547",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Load a PDF from the 6th page onward, clean, and split into chunks\n",
        "\n",
        "def load_and_split_pdf(doc_path):\n",
        "    loader = PyPDFLoader(file_path=doc_path, mode=\"page\")\n",
        "    all_pages = loader.load()\n",
        "    relevant_pages = all_pages[5:]\n",
        "    for page in relevant_pages:\n",
        "        page.page_content = clean_text(page.page_content)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1500,\n",
        "        chunk_overlap=250,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
        "    )\n",
        "    return text_splitter.split_documents(relevant_pages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4ab2708b5edf5152",
      "metadata": {
        "id": "4ab2708b5edf5152",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Process multiple PDF documents into cleaned, chunked outputs\n",
        "\n",
        "def process_documents(documents_path_list: list[str]) -> list:\n",
        "    all_processed_chunks = []\n",
        "    for doc_path in documents_path_list:\n",
        "        logger.info(f\"Processing document: {doc_path}\")\n",
        "        try:\n",
        "            single_doc_chunks = load_and_split_pdf(doc_path)\n",
        "            if single_doc_chunks:\n",
        "                all_processed_chunks.extend(single_doc_chunks)\n",
        "                logger.info(f\"Successfully processed and extracted {len(single_doc_chunks)} chunks from {doc_path}\")\n",
        "            else:\n",
        "                logger.warning(f\"No relevant chunks found in {doc_path}.\")\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"File not found: {doc_path}. Please check the file path.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing document {doc_path}: {e}\")\n",
        "    return all_processed_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "de142ddf38b6c5e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de142ddf38b6c5e3",
        "outputId": "f3960f87-299c-4285-92f8-79c5b558e3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-05-17 01:08:27.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_documents\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mProcessing document: /content/docs/Introduction to Data Science F-CSIT359-S.pdf\u001b[0m\n",
            "\u001b[32m2025-05-17 01:08:32.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_documents\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mSuccessfully processed and extracted 961 chunks from /content/docs/Introduction to Data Science F-CSIT359-S.pdf\u001b[0m\n",
            "\u001b[32m2025-05-17 01:08:32.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_documents\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mProcessing document: /content/docs/Introduction to E-Governance F-CSIT326 S.pdf\u001b[0m\n",
            "\u001b[32m2025-05-17 01:08:37.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_documents\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mSuccessfully processed and extracted 978 chunks from /content/docs/Introduction to E-Governance F-CSIT326 S.pdf\u001b[0m\n",
            "\u001b[32m2025-05-17 01:08:37.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_documents\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mProcessing document: /content/docs/Blockchain Technologies F-CSIT358-B.pdf\u001b[0m\n",
            "\u001b[32m2025-05-17 01:08:41.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_documents\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mSuccessfully processed and extracted 979 chunks from /content/docs/Blockchain Technologies F-CSIT358-B.pdf\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "documents = process_documents(documents_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea554aef-a1c8-4457-bfff-e71d0c2f21fe",
      "metadata": {
        "id": "ea554aef-a1c8-4457-bfff-e71d0c2f21fe"
      },
      "source": [
        "# 2. Create Embeddings ðŸ§ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "JS_Vhjg2LieN",
      "metadata": {
        "id": "JS_Vhjg2LieN"
      },
      "outputs": [],
      "source": [
        "# Creating the embeddding function here\n",
        "\n",
        "# Also using the latest embdedding function here :)\n",
        "# embedding_fn = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\")\n",
        "# Resource has been exhausted, its not free switching to a free one :(\n",
        "\n",
        "embedding_fn = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "TFJFXEryLLnE",
      "metadata": {
        "id": "TFJFXEryLLnE",
        "outputId": "b93e51f0-5048-4805-e7cf-96a0af365fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "Error getting collection: Missing field: [Missing metadata segment]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4d515191f097>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Creating the memory vector database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvectordb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m   1235\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             ):\n\u001b[0;32m-> 1187\u001b[0;31m                 chroma_collection.add_texts(\n\u001b[0m\u001b[1;32m   1188\u001b[0m                     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0mids_with_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_empty_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                     self._collection.upsert(\n\u001b[0m\u001b[1;32m    552\u001b[0m                         \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_with_metadatas\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mupsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         self._client._upsert(\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0mcollection_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupsert_request\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/rust.py\u001b[0m in \u001b[0;36m_upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mdatabase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_DATABASE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     ) -> bool:\n\u001b[0;32m--> 471\u001b[0;31m         return self.bindings.upsert(\n\u001b[0m\u001b[1;32m    472\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Error getting collection: Missing field: [Missing metadata segment]"
          ]
        }
      ],
      "source": [
        "persist_directory = 'db'\n",
        "if not os.path.exists(persist_directory):\n",
        "    os.makedirs(persist_directory, exist_ok=True)\n",
        "\n",
        "# Creating the memory vector database\n",
        "vectordb = Chroma.from_documents(documents,embedding=embedding_fn,persist_directory=persist_directory)\n",
        "\n",
        "# vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pIPG-66CQrtv",
      "metadata": {
        "id": "pIPG-66CQrtv"
      },
      "source": [
        "### Creating the vector retreiver"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ikVNEoPix67a"
      },
      "id": "ikVNEoPix67a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4yLobB_VQwKP",
      "metadata": {
        "collapsed": true,
        "id": "4yLobB_VQwKP"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 7})\n",
        "retrieved_docs = retriever.invoke(\"What is benefit of Bitcoin?\")\n",
        "logger.debug(retrieved_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i1YkSgYKRl93",
      "metadata": {
        "id": "i1YkSgYKRl93"
      },
      "source": [
        "### Invoking the LLM to structure and return the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aDFtNcu6R8eh",
      "metadata": {
        "id": "aDFtNcu6R8eh"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Initializing the Gemini LLM instance\")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",temperature=0.3, max_tokens=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hEjXrGWUS4Zy",
      "metadata": {
        "id": "hEjXrGWUS4Zy"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question.\"\n",
        "    \"If you don't know the answer, say that you don't know.\"\n",
        "    \"Use three sentences maximum and keep the answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        "    \"Below are some examples showing a question and answer format:\"\n",
        "    \"\"\"\n",
        "    Question: The use of e-governance helps make all functions of the ____________ transparent.\n",
        "              Question 1\n",
        "              Answer a. retail.\n",
        "              b. business.\n",
        "              c. Both A & B.\n",
        "              d. None of the above.\n",
        "\n",
        "    Answer:  b. business.\n",
        "                Because e-governance is a system that uses technology to improve the efficiency and transparency of government operations, making it easier for citizens to access information and services.\n",
        "\n",
        "\n",
        "    Question: __________does not directly links to accountability.\n",
        "\n",
        "              Question 2Answer\n",
        "              a.\n",
        "              Opaque.\n",
        "              b.\n",
        "              Transparency.\n",
        "              c.\n",
        "              Both A & B.\n",
        "              d.\n",
        "              None of the above.\n",
        "\n",
        "    Answer:  a. Opaque.\n",
        "                Because Opaque means not able to be seen through; not transparent. In the context of accountability, it suggests a lack of clarity or openness in processes or decisions, which does not directly link to accountability.\n",
        "\n",
        "\n",
        "\n",
        "    Now, Answer the user question correctly given the example formats above:\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vgmr5h1xTA74",
      "metadata": {
        "id": "vgmr5h1xTA74"
      },
      "outputs": [],
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3NSN_jgiTC94",
      "metadata": {
        "id": "3NSN_jgiTC94"
      },
      "outputs": [],
      "source": [
        "response = rag_chain.invoke({\"input\": \"\"\"\n",
        "What is the advantage of Data Science?\n",
        "\n",
        "Question 1Answer\n",
        "a.\n",
        "It is blurry\n",
        "\n",
        "b.\n",
        "Gives good salary\n",
        "\n",
        "c.\n",
        "A person can work on different approach\n",
        "\n",
        "d.\n",
        "It is very good defined\n",
        "\"\"\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93e9b98-c290-42dd-9991-85432944135d",
      "metadata": {
        "id": "b93e9b98-c290-42dd-9991-85432944135d"
      },
      "source": [
        "## 3. Creating the UI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66425af824fb172d",
      "metadata": {
        "id": "66425af824fb172d"
      },
      "source": [
        "### 3.1. Using Streamlit"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}