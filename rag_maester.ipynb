{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f875d2fa-8df2-4951-9bfb-87b71c8b5e1b",
   "metadata": {},
   "source": [
    "![](assets/img/image.png)\n",
    "# RAG Maester\n",
    "**Your AI Scholar**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73deb7f857b0d1c8",
   "metadata": {},
   "source": [
    "Welcome to **RAG Maester**, an Academic AI assistant designed to support academic excellence.\n",
    "It leverages **Retrieval Augmented Generation (RAG)** to meticulously search its knowledge base and craft well-informed responses, designed to assist with university assignments and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42ccfa-7fdf-44ff-9e93-aabc88c74453",
   "metadata": {},
   "source": [
    "#### Imports all required librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e896eba-e5de-45a3-89e2-0de1167d6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac4f17b1ecc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d5fee-8773-47c9-a070-80aae4524516",
   "metadata": {},
   "source": [
    "### 1. Upload and Ingest Documents ðŸ“„ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42271c07-9995-4d18-a2e4-5b2abb3c7088",
   "metadata": {},
   "source": [
    "#### Scan the docs directory for all available documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c0b93-1cf3-409f-993c-10b0bf91b43f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fetch_all_docs(docs_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Lists all files in the specified directory and returns their fully qualified paths.\n",
    "\n",
    "    This function iterates through all entries in the given directory. If an entry\n",
    "    is identified as a file (as opposed to a subdirectory), its complete,\n",
    "    fully qualified path is constructed and added to the returned list.\n",
    "\n",
    "    Args:\n",
    "        docs_path (str): The path to the directory from which to fetch file paths.\n",
    "                         Example: \"/path/to/your/documents\" or \"data/reports\".\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of fully qualified paths for all files found directly\n",
    "                   within the specified directory.\n",
    "                   Returns an empty list if:\n",
    "                   - The `docs_path` does not exist or is not a directory.\n",
    "                   - The directory contains no files.\n",
    "                   - An OS-level error occurs (e.g., permission denied).\n",
    "\n",
    "    Raises:\n",
    "        # This function, as written, handles common OS errors internally and returns\n",
    "        # an empty list. If you prefer to raise exceptions, the try-except block\n",
    "        # can be modified.\n",
    "    \"\"\"\n",
    "    docs_list = []  # Initialize an empty list to store the full paths of files\n",
    "\n",
    "    # First, check if the provided docs_path is actually a directory and exists\n",
    "    if not os.path.isdir(docs_path):\n",
    "        print(f\"Warning: The path '{docs_path}' is not a valid directory or does not exist.\")\n",
    "        return []  # Return an empty list if the path is not a directory\n",
    "\n",
    "    try:\n",
    "        # List all items (files and directories) in the given docs_path\n",
    "        for item_name in os.listdir(docs_path):\n",
    "            # Construct the fully qualified path for the current item\n",
    "            item_full_path = os.path.join(docs_path, item_name)\n",
    "\n",
    "            # Check if the constructed path points to a file (and not a directory)\n",
    "            if os.path.isfile(item_full_path):\n",
    "                # If it's a file, add its fully qualified path to our list\n",
    "                docs_list.append(item_full_path)\n",
    "    except OSError as e:\n",
    "        # Handle potential OS-level errors, such as permission denied\n",
    "        print(f\"Error accessing or reading directory '{docs_path}': {e}\")\n",
    "        return [] # Return an empty list in case of such errors\n",
    "\n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f7cfa-8f7f-4dba-8a1d-36b2b30730bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching all documents from the docs directory\n",
    "documents_list = fetch_all_docs(os.getcwd() + \"/docs\")\n",
    "\n",
    "print(\"Total number of documents found: \", len(documents_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc996afb95cabb",
   "metadata": {},
   "source": [
    "#### Split the documents into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4602da5-1e12-461c-ab02-dedd3101f839",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Method to clean the text\n",
    "# This function removes common watermark or repeated footer content\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes common watermark or repeated footer content.\n",
    "    \"\"\"\n",
    "    removal_phrases = [\n",
    "        \"(c) Amity University Online\",\n",
    "        \"Notes\",\n",
    "        \"Amity Directorate of Distance & Online Education\",\n",
    "        \"Introduction to E-Governance\"\n",
    "    ]\n",
    "    for phrase in removal_phrases:\n",
    "        text = text.replace(phrase, \"\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ca8da-23c6-4f4f-92a8-0fdb1a25b547",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_and_split_pdf(doc_path):\n",
    "    \"\"\"\n",
    "    Loads a PDF from the 6th page onward, cleans watermark text, and splits efficiently.\n",
    "\n",
    "    Args:\n",
    "        doc_path (str): Full path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned and chunked documents.\n",
    "    \"\"\"\n",
    "    # Load all pages\n",
    "    loader = PyPDFLoader(file_path=doc_path, mode=\"page\")\n",
    "    all_pages = loader.load()\n",
    "\n",
    "    # Ignore the first 5 pages\n",
    "    relevant_pages = all_pages[5:]\n",
    "\n",
    "    # Clean watermark from each page\n",
    "    for page in relevant_pages:\n",
    "        page.page_content = clean_text(page.page_content)\n",
    "\n",
    "    # Use a more efficient splitter config\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=250,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],  # smart fallback separator list\n",
    "    )\n",
    "\n",
    "    # Split and return\n",
    "    return text_splitter.split_documents(relevant_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2708b5edf5152",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_documents(documents_path_list: list[str]) -> list:\n",
    "    \"\"\"\n",
    "    Processes a list of PDF document paths.\n",
    "    For each document, it loads content from the 6th page onward,\n",
    "    cleans it, and splits it into chunks using the `load_and_split_pdf` function.\n",
    "\n",
    "    Args:\n",
    "        documents_path_list (list[str]): A list of full file paths to PDF documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A single list containing all cleaned and chunked Document objects\n",
    "              from all successfully processed PDF files.\n",
    "    \"\"\"\n",
    "    all_processed_chunks = []  # Initialize an empty list to store chunks from all documents\n",
    "\n",
    "    # Iterate over each document path in the provided list\n",
    "    for doc_path in documents_path_list:\n",
    "        print(f\"Processing document: {doc_path}\")\n",
    "        try:\n",
    "            # Call the existing function to process a single document\n",
    "            single_doc_chunks = load_and_split_pdf(doc_path)\n",
    "\n",
    "            # Add the chunks from the current document to the main list\n",
    "            if single_doc_chunks: # Ensure there are chunks to add\n",
    "                all_processed_chunks.extend(single_doc_chunks)\n",
    "                print(f\"Successfully processed and extracted {len(single_doc_chunks)} chunks from {doc_path}\")\n",
    "            else:\n",
    "                print(f\"No relevant chunks extracted from {doc_path} (e.g., too few pages or empty content after cleaning).\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Document not found at {doc_path}. Skipping this document.\")\n",
    "        except Exception as e:\n",
    "            # Catch any other errors during the processing of a single document\n",
    "            print(f\"Error processing document {doc_path}: {e}. Skipping this document.\")\n",
    "\n",
    "    return all_processed_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de142ddf38b6c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = process_documents(documents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea554aef-a1c8-4457-bfff-e71d0c2f21fe",
   "metadata": {},
   "source": [
    "### 2. Create Embeddings ðŸ§ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e9b98-c290-42dd-9991-85432944135d",
   "metadata": {},
   "source": [
    "## 3. Creating the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66425af824fb172d",
   "metadata": {},
   "source": [
    "### 3.1. Using Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1690396561482e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# --- Setup ---\u001b[39;00m\n\u001b[0;32m      8\u001b[0m st\u001b[38;5;241m.\u001b[39mset_page_config(page_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG Maester\u001b[39m\u001b[38;5;124m\"\u001b[39m, layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwide\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e5d35-d91e-47f5-8aea-b1a9a0f25c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run streamlit_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059cf113-fae9-441d-a1ba-e84c49b8931c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
